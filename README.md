# Обработчик текста

Программа представляет обработчик текста (в данном случае текст романа "Анна Коренена").
В программе присутствует два модуля: `main.py` и `text_handler_and_statistic.py`. В главном файле `main.py` испослняются
функции импортированные из файла `text_handler_and_statistic.py`, которые открывают, читают, обрабатывает и считает 
статистику.

Так же присутствует сам файл текст романа "Анна Коренена" и файл `Анна Коренена.postman_collection.json`, которые 
содержит HTML-запросы для mock-сервера https://66095c000f324a9a28832d7e.mockapi.io/state.

## Функции

### `text_handler_and_statistic.py`

В модул импортируется объект `Counter` для подсчета токенов. 

Установлена библиотека `nltk`, которая является набором инструментов для обработки текста. 

#### 

1. Импорт:

   * `Counter` из `collections`: Используется для подсчета вхождений элементов.
   * `nltk`: Библиотека для обработки естественного языка (NLP).
   * `stopword` и `word_tokenize` из `nltk.corpus` и `nltk.tokenize`: функции для удаления стоп-слов(слова, предлоги) и разметки слов.

2. Загрузка ресурсов NLTK:

   * nltk.download("стоп-слова"): Загружает список стоп-слов для русского языка.

3. Функция load_text:

   * Принимает имя файла (file_name) в качестве входных данных.
   * Открывает файл в режиме чтения в кодировке UTF-8 с помощью контекстного менеджера (с функцией open).
   * Считывает все содержимое файла и возвращает его.
   * Включает обработку ошибки FileNotFoundError и выводит сообщение об ошибке.

4. функция clean_text:

Принимает текстовые данные в качестве входных данных.
Использует параметр word_tokenize с параметром russian language для разбиения текста на отдельные слова (лексемы).
Преобразует все лексемы в нижний регистр.
Отфильтровывает лексемы, которые не являются буквенно-цифровыми (буквы и цифры), используя список.
Возвращает очищенный список токенов.
5. функция remove_stopwords:

Принимает список токенов в качестве входных данных.
Создает набор стоп-слов из списка русских стоп-слов, используя stopwords.words("русский").
Использует понимание списка для фильтрации лексем, которые присутствуют в списке стоп-слов.
Возвращает список лексем после удаления стоп-слов.
6. функция count_words:

Принимает список токенов в качестве входных данных.
Использует счетчик из коллекций для создания объекта, подобного словарю, который подсчитывает вхождения каждого уникального токена.
Возвращает объект счетчика с количеством слов.
7. Функция display_top_words:

Принимает в качестве входных данных объект счетчика (counted_tokens) и необязательный параметр num_top (по умолчанию 10).
Использует метод most_common счетчика для получения наиболее часто встречающихся лексем и их количества.
По умолчанию он возвращает 10 самых часто встречающихся слов (можно изменить с помощью num_top).